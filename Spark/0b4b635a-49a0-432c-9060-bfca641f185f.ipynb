{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f0ff45",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Описание-данных\" data-toc-modified-id=\"Описание-данных-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Описание данных</a></span></li><li><span><a href=\"#Загрузка-библиотек-и-данных\" data-toc-modified-id=\"Загрузка-библиотек-и-данных-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Загрузка библиотек и данных</a></span></li><li><span><a href=\"#Предобработка\" data-toc-modified-id=\"Предобработка-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Предобработка</a></span></li><li><span><a href=\"#Обучение-модели\" data-toc-modified-id=\"Обучение-модели-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Обучение модели</a></span><ul class=\"toc-item\"><li><span><a href=\"#LogisticRegression-все-данные\" data-toc-modified-id=\"LogisticRegression-все-данные-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>LogisticRegression все данные</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65354a74",
   "metadata": {},
   "source": [
    "## Описание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f56a9",
   "metadata": {},
   "source": [
    "В проекте вам нужно обучить модель линейной регрессии на данных о жилье в Калифорнии в 1990 году"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb3d14",
   "metadata": {},
   "source": [
    "В колонках датасета содержатся следующие данные:\n",
    "- longitude — широта;\n",
    "- latitude — долгота;\n",
    "- housing_median_age — медианный возраст жителей жилого массива;\n",
    "- total_rooms — общее количество комнат в домах жилого массива;\n",
    "- total_bedrooms — общее количество спален в домах жилого массива;\n",
    "- population — количество человек, которые проживают в жилом массиве;\n",
    "- households — количество домовладений в жилом массиве;\n",
    "- median_income — медианный доход жителей жилого массива;\n",
    "- median_house_value — медианная стоимость дома в жилом массиве;\n",
    "- ocean_proximity — близость к океану."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadd48a",
   "metadata": {},
   "source": [
    "На основе данных нужно предсказать медианную стоимость дома в жилом массиве — median_house_value. Обучите модель и сделайте предсказания на тестовой выборке. Для оценки качества модели используйте метрики RMSE, MAE и R2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8132d9",
   "metadata": {},
   "source": [
    "## Загрузка библиотек и данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647a38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "if int(pyspark_version[:1]) == 3:\n",
    "    from pyspark.ml.feature import OneHotEncoder    \n",
    "elif int(pyspark_version[:1]) == 2:\n",
    "    from pyspark.ml.feature import OneHotEncodeEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14220c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 15364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e70a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"EDA California Housing\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f303cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|          269700.0|       NEAR BAY|\n",
      "|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|          299200.0|       NEAR BAY|\n",
      "|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|\n",
      "|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|          226700.0|       NEAR BAY|\n",
      "|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|          261100.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|          281500.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|          241800.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|          213500.0|       NEAR BAY|\n",
      "|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|          159200.0|       NEAR BAY|\n",
      "|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|\n",
      "|  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|          152500.0|       NEAR BAY|\n",
      "|  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|          155500.0|       NEAR BAY|\n",
      "|  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|          158700.0|       NEAR BAY|\n",
      "|  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|          162900.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_housing = spark.read.load('./housing.csv', format=\"csv\", sep=\",\", inferSchema=True, header=\"true\")\n",
    "df_housing.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63014d24",
   "metadata": {},
   "source": [
    "## Предобработка  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f91ab",
   "metadata": {},
   "source": [
    "Проверим данные на пропуски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b9e3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>20640</td>\n",
       "      <td>20640</td>\n",
       "      <td>20640</td>\n",
       "      <td>20640</td>\n",
       "      <td>20433</td>\n",
       "      <td>20640</td>\n",
       "      <td>20640</td>\n",
       "      <td>20640</td>\n",
       "      <td>20640</td>\n",
       "      <td>20640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>-119.56970445736148</td>\n",
       "      <td>35.6318614341087</td>\n",
       "      <td>28.639486434108527</td>\n",
       "      <td>2635.7630813953488</td>\n",
       "      <td>537.8705525375618</td>\n",
       "      <td>1425.4767441860465</td>\n",
       "      <td>499.5396802325581</td>\n",
       "      <td>3.8706710029070246</td>\n",
       "      <td>206855.81690891474</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>2.003531723502584</td>\n",
       "      <td>2.135952397457101</td>\n",
       "      <td>12.58555761211163</td>\n",
       "      <td>2181.6152515827944</td>\n",
       "      <td>421.38507007403115</td>\n",
       "      <td>1132.46212176534</td>\n",
       "      <td>382.3297528316098</td>\n",
       "      <td>1.899821717945263</td>\n",
       "      <td>115395.61587441359</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-124.35</td>\n",
       "      <td>32.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4999</td>\n",
       "      <td>14999.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>-114.31</td>\n",
       "      <td>41.95</td>\n",
       "      <td>52.0</td>\n",
       "      <td>39320.0</td>\n",
       "      <td>6445.0</td>\n",
       "      <td>35682.0</td>\n",
       "      <td>6082.0</td>\n",
       "      <td>15.0001</td>\n",
       "      <td>500001.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary            longitude           latitude  housing_median_age  \\\n",
       "0   count                20640              20640               20640   \n",
       "1    mean  -119.56970445736148   35.6318614341087  28.639486434108527   \n",
       "2  stddev    2.003531723502584  2.135952397457101   12.58555761211163   \n",
       "3     min              -124.35              32.54                 1.0   \n",
       "4     max              -114.31              41.95                52.0   \n",
       "\n",
       "          total_rooms      total_bedrooms          population  \\\n",
       "0               20640               20433               20640   \n",
       "1  2635.7630813953488   537.8705525375618  1425.4767441860465   \n",
       "2  2181.6152515827944  421.38507007403115    1132.46212176534   \n",
       "3                 2.0                 1.0                 3.0   \n",
       "4             39320.0              6445.0             35682.0   \n",
       "\n",
       "          households       median_income  median_house_value ocean_proximity  \n",
       "0              20640               20640               20640           20640  \n",
       "1  499.5396802325581  3.8706710029070246  206855.81690891474            None  \n",
       "2  382.3297528316098   1.899821717945263  115395.61587441359            None  \n",
       "3                1.0              0.4999             14999.0       <1H OCEAN  \n",
       "4             6082.0             15.0001            500001.0      NEAR OCEAN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_housing.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e6c97",
   "metadata": {},
   "source": [
    "В столбце `total_bedrooms` есть пропуски, заполнять их чем попало не получится, нужно просто удалить, учитывая что это всего 200 строк, примерно 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ba06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = df_housing.dropna(how = 'any', subset = 'total_bedrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc6fddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "      <td>20433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>-119.57068859198068</td>\n",
       "      <td>35.63322125972706</td>\n",
       "      <td>28.633093525179856</td>\n",
       "      <td>2636.5042333480155</td>\n",
       "      <td>537.8705525375618</td>\n",
       "      <td>1424.9469485635982</td>\n",
       "      <td>499.43346547251997</td>\n",
       "      <td>3.8711616013312273</td>\n",
       "      <td>206864.41315519012</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>2.003577890751096</td>\n",
       "      <td>2.1363476663779872</td>\n",
       "      <td>12.591805202182835</td>\n",
       "      <td>2185.269566977601</td>\n",
       "      <td>421.38507007403115</td>\n",
       "      <td>1133.2084897449597</td>\n",
       "      <td>382.2992258828481</td>\n",
       "      <td>1.899291249306247</td>\n",
       "      <td>115435.66709858322</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-124.35</td>\n",
       "      <td>32.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4999</td>\n",
       "      <td>14999.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>-114.31</td>\n",
       "      <td>41.95</td>\n",
       "      <td>52.0</td>\n",
       "      <td>39320.0</td>\n",
       "      <td>6445.0</td>\n",
       "      <td>35682.0</td>\n",
       "      <td>6082.0</td>\n",
       "      <td>15.0001</td>\n",
       "      <td>500001.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary            longitude            latitude  housing_median_age  \\\n",
       "0   count                20433               20433               20433   \n",
       "1    mean  -119.57068859198068   35.63322125972706  28.633093525179856   \n",
       "2  stddev    2.003577890751096  2.1363476663779872  12.591805202182835   \n",
       "3     min              -124.35               32.54                 1.0   \n",
       "4     max              -114.31               41.95                52.0   \n",
       "\n",
       "          total_rooms      total_bedrooms          population  \\\n",
       "0               20433               20433               20433   \n",
       "1  2636.5042333480155   537.8705525375618  1424.9469485635982   \n",
       "2   2185.269566977601  421.38507007403115  1133.2084897449597   \n",
       "3                 2.0                 1.0                 3.0   \n",
       "4             39320.0              6445.0             35682.0   \n",
       "\n",
       "           households       median_income  median_house_value ocean_proximity  \n",
       "0               20433               20433               20433           20433  \n",
       "1  499.43346547251997  3.8711616013312273  206864.41315519012            None  \n",
       "2   382.2992258828481   1.899291249306247  115435.66709858322            None  \n",
       "3                 1.0              0.4999             14999.0       <1H OCEAN  \n",
       "4              6082.0             15.0001            500001.0      NEAR OCEAN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_housing.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bfda9",
   "metadata": {},
   "source": [
    "Теперь можно перевести категориальные значения в понятные для модели цифры через OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fe3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCols=['ocean_proximity'], \n",
    "                        outputCols=['ocean_proximity_idx']) \n",
    "df_housing = indexer.fit(df_housing).transform(df_housing)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['ocean_proximity_idx'],\n",
    "                        outputCols=['ocean_proximity_ohe'])\n",
    "df_housing = encoder.fit(df_housing).transform(df_housing)\n",
    "\n",
    "categorical_assembler =VectorAssembler(inputCols=['ocean_proximity_ohe'],\n",
    "                                        outputCol=\"categorical_features\")\n",
    "df_housing = categorical_assembler.transform(df_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eca5328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>ocean_proximity_idx</th>\n",
       "      <th>categorical_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14862</th>\n",
       "      <td>-117.03</td>\n",
       "      <td>32.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1796.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>2.8750</td>\n",
       "      <td>161200.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>-119.85</td>\n",
       "      <td>36.84</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2272.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>840.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>8.9669</td>\n",
       "      <td>213900.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15565</th>\n",
       "      <td>-122.43</td>\n",
       "      <td>37.79</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2459.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>957.0</td>\n",
       "      <td>2.6782</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19210</th>\n",
       "      <td>-120.93</td>\n",
       "      <td>37.74</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>2.3023</td>\n",
       "      <td>91900.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8894</th>\n",
       "      <td>-118.44</td>\n",
       "      <td>33.98</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18132.0</td>\n",
       "      <td>5419.0</td>\n",
       "      <td>7431.0</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>5.3359</td>\n",
       "      <td>500001.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "14862    -117.03     32.77                34.0       1796.0           428.0   \n",
       "2201     -119.85     36.84                12.0       2272.0           304.0   \n",
       "15565    -122.43     37.79                24.0       2459.0          1001.0   \n",
       "19210    -120.93     37.74                37.0       1956.0           402.0   \n",
       "8894     -118.44     33.98                21.0      18132.0          5419.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "14862       918.0       424.0         2.8750            161200.0   \n",
       "2201        840.0       305.0         8.9669            213900.0   \n",
       "15565      1362.0       957.0         2.6782            450000.0   \n",
       "19210      1265.0       397.0         2.3023             91900.0   \n",
       "8894       7431.0      4930.0         5.3359            500001.0   \n",
       "\n",
       "      ocean_proximity  ocean_proximity_idx categorical_features  \n",
       "14862       <1H OCEAN                  0.0                [0.0]  \n",
       "2201           INLAND                  1.0                [1.0]  \n",
       "15565        NEAR BAY                  3.0                [3.0]  \n",
       "19210          INLAND                  1.0                [1.0]  \n",
       "8894        <1H OCEAN                  0.0                [0.0]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_housing.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557654f7",
   "metadata": {},
   "source": [
    "Теперь можно и числовые столбцы привести к одному размеру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8608595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_assembler = VectorAssembler(inputCols=['longitude',\n",
    "                                                 'latitude',\n",
    "                                                 'housing_median_age',\n",
    "                                                 'total_rooms',\n",
    "                                                 'total_bedrooms',\n",
    "                                                 'population',\n",
    "                                                 'households',\n",
    "                                                 'median_income',],\n",
    "                                      outputCol=\"numerical_features\")\n",
    "df_housing = numerical_assembler.transform(df_housing) \n",
    "standardScaler = StandardScaler(inputCol='numerical_features',\n",
    "                                outputCol=\"numerical_features_scaled\")\n",
    "\n",
    "df_housing = standardScaler.fit(df_housing).transform(df_housing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f1aeee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>ocean_proximity_idx</th>\n",
       "      <th>categorical_features</th>\n",
       "      <th>numerical_features</th>\n",
       "      <th>numerical_features_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>-122.08</td>\n",
       "      <td>37.61</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>3.7931</td>\n",
       "      <td>203900.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[-122.08, 37.61, 26.0, 2261.0, 443.0, 1039.0, ...</td>\n",
       "      <td>[-60.93099777330593, 17.604812452537214, 2.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>-118.26</td>\n",
       "      <td>33.93</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.5256</td>\n",
       "      <td>95400.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[-118.26, 33.93, 36.0, 1102.0, 247.0, 702.0, 2...</td>\n",
       "      <td>[-59.02440855726704, 15.882246384328308, 2.859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14197</th>\n",
       "      <td>-117.16</td>\n",
       "      <td>32.72</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>1.2668</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[-117.16, 32.72, 27.0, 1245.0, 471.0, 653.0, 4...</td>\n",
       "      <td>[-58.47539072018777, 15.315859171683533, 2.144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10124</th>\n",
       "      <td>-117.88</td>\n",
       "      <td>33.87</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1302.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>5.6409</td>\n",
       "      <td>190900.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[-117.88, 33.87, 35.0, 1919.0, 349.0, 1302.0, ...</td>\n",
       "      <td>[-58.834747849912375, 15.854161067998815, 2.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>-118.44</td>\n",
       "      <td>34.02</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2242.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>4.0429</td>\n",
       "      <td>500001.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[-118.44, 34.02, 32.0, 2242.0, 490.0, 921.0, 4...</td>\n",
       "      <td>[-59.11424783969819, 15.92437435882255, 2.5413...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "823      -122.08     37.61                26.0       2261.0           443.0   \n",
       "5137     -118.26     33.93                36.0       1102.0           247.0   \n",
       "14197    -117.16     32.72                27.0       1245.0           471.0   \n",
       "10124    -117.88     33.87                35.0       1919.0           349.0   \n",
       "5359     -118.44     34.02                32.0       2242.0           490.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "823        1039.0       395.0         3.7931            203900.0   \n",
       "5137        702.0       225.0         1.5256             95400.0   \n",
       "14197       653.0       451.0         1.2668            225000.0   \n",
       "10124      1302.0       345.0         5.6409            190900.0   \n",
       "5359        921.0       461.0         4.0429            500001.0   \n",
       "\n",
       "      ocean_proximity  ocean_proximity_idx categorical_features  \\\n",
       "823          NEAR BAY                  3.0                [3.0]   \n",
       "5137        <1H OCEAN                  0.0                [0.0]   \n",
       "14197      NEAR OCEAN                  2.0                [2.0]   \n",
       "10124       <1H OCEAN                  0.0                [0.0]   \n",
       "5359        <1H OCEAN                  0.0                [0.0]   \n",
       "\n",
       "                                      numerical_features  \\\n",
       "823    [-122.08, 37.61, 26.0, 2261.0, 443.0, 1039.0, ...   \n",
       "5137   [-118.26, 33.93, 36.0, 1102.0, 247.0, 702.0, 2...   \n",
       "14197  [-117.16, 32.72, 27.0, 1245.0, 471.0, 653.0, 4...   \n",
       "10124  [-117.88, 33.87, 35.0, 1919.0, 349.0, 1302.0, ...   \n",
       "5359   [-118.44, 34.02, 32.0, 2242.0, 490.0, 921.0, 4...   \n",
       "\n",
       "                               numerical_features_scaled  \n",
       "823    [-60.93099777330593, 17.604812452537214, 2.064...  \n",
       "5137   [-59.02440855726704, 15.882246384328308, 2.859...  \n",
       "14197  [-58.47539072018777, 15.315859171683533, 2.144...  \n",
       "10124  [-58.834747849912375, 15.854161067998815, 2.77...  \n",
       "5359   [-59.11424783969819, 15.92437435882255, 2.5413...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_housing.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129540d",
   "metadata": {},
   "source": [
    "Сибираем все вместе для будущего обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aea04a3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_features = ['categorical_features','numerical_features_scaled']\n",
    "\n",
    "final_assembler = VectorAssembler(inputCols=all_features, \n",
    "                                  outputCol='features') \n",
    "df_housing = final_assembler.transform(df_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63bfe5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3.0, -61.00586384199856, 17.731196376019934, ...</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3.0, -61.00087277075238, 17.721834603910104, ...</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.0, -61.010854913244735, 17.717153717855187,...</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3.0, -61.01584598449091, 17.717153717855187, ...</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3.0, -61.01584598449091, 17.717153717855187, ...</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  median_house_value\n",
       "0  [3.0, -61.00586384199856, 17.731196376019934, ...            452600.0\n",
       "1  [3.0, -61.00087277075238, 17.721834603910104, ...            358500.0\n",
       "2  [3.0, -61.010854913244735, 17.717153717855187,...            352100.0\n",
       "3  [3.0, -61.01584598449091, 17.717153717855187, ...            341300.0\n",
       "4  [3.0, -61.01584598449091, 17.717153717855187, ...            342200.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_housing.select(['features', 'median_house_value']).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76ac9c",
   "metadata": {},
   "source": [
    "Теперь делим на обучающую и проверочную выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31af9ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16225 4208\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = df_housing.randomSplit([.8,.2], seed=RANDOM_SEED)\n",
    "print(train_data.count(), test_data.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ebd409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|median_house_value|\n",
      "+--------------------+------------------+\n",
      "|[3.0,-61.00586384...|          452600.0|\n",
      "|[3.0,-61.00087277...|          358500.0|\n",
      "|[3.0,-61.01085491...|          352100.0|\n",
      "|[3.0,-61.01584598...|          341300.0|\n",
      "|[3.0,-61.01584598...|          342200.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_housing.select('features','median_house_value').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebd85449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+-------------------+--------------------+--------------------+-------------------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|ocean_proximity_idx|categorical_features|  numerical_features|numerical_features_scaled|            features|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+-------------------+--------------------+--------------------+-------------------------+--------------------+\n",
      "|  -124.35|   40.54|              52.0|     1820.0|         300.0|     806.0|     270.0|       3.0147|           94600.0|     NEAR OCEAN|                2.0|               [2.0]|[-124.35,40.54,52...|     [-62.063970946187...|[2.0,-62.06397094...|\n",
      "|   -124.3|    41.8|              19.0|     2672.0|         552.0|    1298.0|     478.0|       1.9797|           85800.0|     NEAR OCEAN|                2.0|               [2.0]|[-124.3,41.8,19.0...|     [-62.039015589956...|[2.0,-62.03901558...|\n",
      "|   -124.3|   41.84|              17.0|     2677.0|         531.0|    1244.0|     456.0|       3.0313|          103600.0|     NEAR OCEAN|                2.0|               [2.0]|[-124.3,41.84,17....|     [-62.039015589956...|[2.0,-62.03901558...|\n",
      "|  -124.27|   40.69|              36.0|     2349.0|         528.0|    1194.0|     465.0|       2.5179|           79000.0|     NEAR OCEAN|                2.0|               [2.0]|[-124.27,40.69,36...|     [-62.024042376218...|[2.0,-62.02404237...|\n",
      "|  -124.26|   40.58|              52.0|     2217.0|         394.0|     907.0|     369.0|       2.3571|          111400.0|     NEAR OCEAN|                2.0|               [2.0]|[-124.26,40.58,52...|     [-62.019051304972...|[2.0,-62.01905130...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+-------------------+--------------------+--------------------+-------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a344e",
   "metadata": {},
   "source": [
    "Отлично, все готово для будущего обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ec2e0",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d0a56",
   "metadata": {},
   "source": [
    "Для обучения модели будем использовать несолько подходов:\n",
    "1. Обучать нразные модели\n",
    "2. Использовать разное количество данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbadfdd",
   "metadata": {},
   "source": [
    "### LogisticRegression все данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f9a5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol='median_house_value', featuresCol='numerical_features_scaled',maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78e3a276",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o258.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 23) (DESKTOP-UD3OFTF executor driver): java.lang.IllegalArgumentException: requirement failed: 14563 x 500002 dense matrix is too large to allocate\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.DenseMatrix$.zeros(Matrices.scala:490)\r\n\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:112)\r\n\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\r\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\r\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\r\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\r\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\r\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\r\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1000)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:629)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:496)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: 14563 x 500002 dense matrix is too large to allocate\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.DenseMatrix$.zeros(Matrices.scala:490)\r\n\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:112)\r\n\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\r\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\r\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32ma:\\Practicum\\Spark\\0b4b635a-49a0-432c-9060-bfca641f185f.ipynb Ячейка 35\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/a%3A/Practicum/Spark/0b4b635a-49a0-432c-9060-bfca641f185f.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39;49mfit(train_data)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\pyspark\\ml\\wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[1;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\pyspark\\ml\\wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o258.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 23) (DESKTOP-UD3OFTF executor driver): java.lang.IllegalArgumentException: requirement failed: 14563 x 500002 dense matrix is too large to allocate\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.DenseMatrix$.zeros(Matrices.scala:490)\r\n\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:112)\r\n\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\r\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\r\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\r\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\r\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\r\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\r\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1000)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:629)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:496)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: 14563 x 500002 dense matrix is too large to allocate\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.linalg.DenseMatrix$.zeros(Matrices.scala:490)\r\n\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:112)\r\n\tat org.apache.spark.ml.optim.aggregator.MultinomialLogisticBlockAggregator.add(MultinomialLogisticBlockAggregator.scala:45)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.$anonfun$calculate$1(RDDLossFunction.scala:59)\r\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\r\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "model = lr.fit(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd441611",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"Survived\", \"prediction\").show(10) "
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 15611,
    "start_time": "2022-11-10T11:27:42.947Z"
   },
   {
    "duration": 9145,
    "start_time": "2022-11-10T11:28:21.698Z"
   },
   {
    "duration": 197,
    "start_time": "2022-11-10T11:31:28.451Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-10T11:31:36.284Z"
   },
   {
    "duration": 47,
    "start_time": "2022-11-12T08:18:07.929Z"
   },
   {
    "duration": 15769,
    "start_time": "2022-11-12T08:18:12.698Z"
   },
   {
    "duration": 9786,
    "start_time": "2022-11-12T08:18:28.469Z"
   },
   {
    "duration": 99,
    "start_time": "2022-11-12T08:18:38.256Z"
   },
   {
    "duration": 6300,
    "start_time": "2022-11-12T08:18:38.357Z"
   },
   {
    "duration": 93,
    "start_time": "2022-11-12T08:23:57.860Z"
   },
   {
    "duration": 33,
    "start_time": "2022-11-12T08:24:31.158Z"
   },
   {
    "duration": 4505,
    "start_time": "2022-11-12T08:24:40.629Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-12T08:28:01.303Z"
   },
   {
    "duration": 322,
    "start_time": "2022-11-12T08:28:27.948Z"
   },
   {
    "duration": 131,
    "start_time": "2022-11-12T08:28:31.523Z"
   },
   {
    "duration": 8,
    "start_time": "2022-11-12T08:31:50.139Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-12T08:32:18.623Z"
   },
   {
    "duration": 34,
    "start_time": "2022-11-12T08:32:21.420Z"
   },
   {
    "duration": 1758,
    "start_time": "2022-11-12T08:32:30.818Z"
   },
   {
    "duration": 879,
    "start_time": "2022-11-12T08:32:38.279Z"
   },
   {
    "duration": 709,
    "start_time": "2022-11-12T08:33:04.466Z"
   },
   {
    "duration": 14869,
    "start_time": "2022-11-12T08:33:05.177Z"
   },
   {
    "duration": 8698,
    "start_time": "2022-11-12T08:33:20.048Z"
   },
   {
    "duration": 110,
    "start_time": "2022-11-12T08:33:28.748Z"
   },
   {
    "duration": 6075,
    "start_time": "2022-11-12T08:33:28.861Z"
   },
   {
    "duration": 2194,
    "start_time": "2022-11-12T08:33:34.938Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-12T08:35:14.782Z"
   },
   {
    "duration": 87,
    "start_time": "2022-11-12T08:36:12.438Z"
   },
   {
    "duration": 438,
    "start_time": "2022-11-12T08:36:23.331Z"
   },
   {
    "duration": 875,
    "start_time": "2022-11-14T08:30:14.163Z"
   },
   {
    "duration": 19579,
    "start_time": "2022-11-14T08:30:17.299Z"
   },
   {
    "duration": 9298,
    "start_time": "2022-11-14T08:30:36.880Z"
   },
   {
    "duration": 3923,
    "start_time": "2022-11-14T08:30:57.786Z"
   },
   {
    "duration": 1412,
    "start_time": "2022-11-14T08:31:46.283Z"
   },
   {
    "duration": 1216,
    "start_time": "2022-11-14T08:32:26.386Z"
   },
   {
    "duration": 739,
    "start_time": "2022-11-15T11:18:30.158Z"
   },
   {
    "duration": 14580,
    "start_time": "2022-11-15T11:18:31.216Z"
   },
   {
    "duration": 8391,
    "start_time": "2022-11-15T11:18:45.799Z"
   },
   {
    "duration": 4006,
    "start_time": "2022-11-15T11:18:54.191Z"
   },
   {
    "duration": 455,
    "start_time": "2022-11-15T11:18:58.199Z"
   },
   {
    "duration": 3866,
    "start_time": "2022-11-15T11:20:12.033Z"
   },
   {
    "duration": 1312,
    "start_time": "2022-11-15T11:23:51.178Z"
   },
   {
    "duration": 2562,
    "start_time": "2022-11-15T11:24:12.262Z"
   },
   {
    "duration": 118,
    "start_time": "2022-11-15T11:25:05.777Z"
   },
   {
    "duration": 116,
    "start_time": "2022-11-15T11:25:21.019Z"
   },
   {
    "duration": 101,
    "start_time": "2022-11-15T11:25:27.513Z"
   },
   {
    "duration": 744,
    "start_time": "2022-11-15T11:25:59.975Z"
   },
   {
    "duration": 15178,
    "start_time": "2022-11-15T11:26:00.721Z"
   },
   {
    "duration": 8585,
    "start_time": "2022-11-15T11:26:15.902Z"
   },
   {
    "duration": 3634,
    "start_time": "2022-11-15T11:26:24.489Z"
   },
   {
    "duration": 4454,
    "start_time": "2022-11-15T11:26:28.124Z"
   },
   {
    "duration": 1501,
    "start_time": "2022-11-15T11:26:32.580Z"
   },
   {
    "duration": 2453,
    "start_time": "2022-11-15T11:26:34.083Z"
   },
   {
    "duration": 70,
    "start_time": "2022-11-15T11:37:55.184Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-15T11:38:03.751Z"
   },
   {
    "duration": 1876,
    "start_time": "2022-11-15T11:38:06.067Z"
   },
   {
    "duration": 2408,
    "start_time": "2022-11-15T11:38:13.279Z"
   },
   {
    "duration": 711,
    "start_time": "2022-11-15T11:42:06.717Z"
   },
   {
    "duration": 14989,
    "start_time": "2022-11-15T11:42:07.430Z"
   },
   {
    "duration": 9270,
    "start_time": "2022-11-15T11:42:22.422Z"
   },
   {
    "duration": 4311,
    "start_time": "2022-11-15T11:42:31.695Z"
   },
   {
    "duration": 14,
    "start_time": "2022-11-15T11:42:36.008Z"
   },
   {
    "duration": 1485,
    "start_time": "2022-11-15T11:42:36.024Z"
   },
   {
    "duration": 2089,
    "start_time": "2022-11-15T11:42:37.511Z"
   },
   {
    "duration": 3537,
    "start_time": "2022-11-15T11:42:39.603Z"
   },
   {
    "duration": 124,
    "start_time": "2022-11-15T11:51:15.518Z"
   },
   {
    "duration": 100,
    "start_time": "2022-11-15T11:51:23.009Z"
   },
   {
    "duration": 653,
    "start_time": "2022-11-15T11:51:32.661Z"
   },
   {
    "duration": 15186,
    "start_time": "2022-11-15T11:51:33.316Z"
   },
   {
    "duration": 8585,
    "start_time": "2022-11-15T11:51:48.505Z"
   },
   {
    "duration": 3997,
    "start_time": "2022-11-15T11:51:57.092Z"
   },
   {
    "duration": 22,
    "start_time": "2022-11-15T11:52:01.092Z"
   },
   {
    "duration": 1992,
    "start_time": "2022-11-15T11:52:01.116Z"
   },
   {
    "duration": 2090,
    "start_time": "2022-11-15T11:52:03.110Z"
   },
   {
    "duration": 2688,
    "start_time": "2022-11-15T11:52:05.202Z"
   },
   {
    "duration": 1110,
    "start_time": "2022-11-15T11:52:07.892Z"
   },
   {
    "duration": 473,
    "start_time": "2022-11-15T11:54:34.506Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-15T11:56:01.054Z"
   },
   {
    "duration": 2734,
    "start_time": "2022-11-15T11:56:09.460Z"
   },
   {
    "duration": 668,
    "start_time": "2022-11-15T13:50:19.335Z"
   },
   {
    "duration": 2,
    "start_time": "2022-11-15T13:50:20.004Z"
   },
   {
    "duration": 14387,
    "start_time": "2022-11-15T13:50:20.008Z"
   },
   {
    "duration": 9201,
    "start_time": "2022-11-15T13:50:34.397Z"
   },
   {
    "duration": 4092,
    "start_time": "2022-11-15T13:50:43.600Z"
   },
   {
    "duration": 21,
    "start_time": "2022-11-15T13:50:47.693Z"
   },
   {
    "duration": 1492,
    "start_time": "2022-11-15T13:50:47.716Z"
   },
   {
    "duration": 2073,
    "start_time": "2022-11-15T13:50:49.210Z"
   },
   {
    "duration": 2863,
    "start_time": "2022-11-15T13:50:51.285Z"
   },
   {
    "duration": 1154,
    "start_time": "2022-11-15T13:50:54.151Z"
   },
   {
    "duration": 584,
    "start_time": "2022-11-15T13:50:55.307Z"
   },
   {
    "duration": 2893,
    "start_time": "2022-11-15T13:50:55.893Z"
   },
   {
    "duration": 2262903,
    "start_time": "2022-11-15T13:50:58.788Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:28:41.693Z"
   },
   {
    "duration": 165889,
    "start_time": "2022-11-15T14:33:06.729Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:35:52.620Z"
   },
   {
    "duration": 19159,
    "start_time": "2022-11-15T14:36:20.449Z"
   },
   {
    "duration": 4528,
    "start_time": "2022-11-15T14:36:48.254Z"
   },
   {
    "duration": 28,
    "start_time": "2022-11-15T14:37:20.081Z"
   },
   {
    "duration": 82725,
    "start_time": "2022-11-15T14:38:52.958Z"
   },
   {
    "duration": 5,
    "start_time": "2022-11-15T14:40:20.587Z"
   },
   {
    "duration": 50,
    "start_time": "2022-11-15T14:40:20.594Z"
   },
   {
    "duration": 53,
    "start_time": "2022-11-15T14:40:20.646Z"
   },
   {
    "duration": 40121,
    "start_time": "2022-11-15T14:40:20.702Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:00.826Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:00.827Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:00.828Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:00.829Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:00.830Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:00.831Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:00.831Z"
   },
   {
    "duration": 768,
    "start_time": "2022-11-15T14:41:08.351Z"
   },
   {
    "duration": 2,
    "start_time": "2022-11-15T14:41:09.967Z"
   },
   {
    "duration": 14870,
    "start_time": "2022-11-15T14:41:10.841Z"
   },
   {
    "duration": 9276,
    "start_time": "2022-11-15T14:41:25.714Z"
   },
   {
    "duration": 4486,
    "start_time": "2022-11-15T14:41:34.992Z"
   },
   {
    "duration": 16,
    "start_time": "2022-11-15T14:41:39.481Z"
   },
   {
    "duration": 1698,
    "start_time": "2022-11-15T14:41:39.499Z"
   },
   {
    "duration": 2286,
    "start_time": "2022-11-15T14:41:41.199Z"
   },
   {
    "duration": 3576,
    "start_time": "2022-11-15T14:41:43.491Z"
   },
   {
    "duration": 1234,
    "start_time": "2022-11-15T14:41:47.076Z"
   },
   {
    "duration": 139,
    "start_time": "2022-11-15T14:41:48.311Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:48.452Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:41:48.453Z"
   },
   {
    "duration": 82,
    "start_time": "2022-11-15T14:42:08.662Z"
   },
   {
    "duration": 3371,
    "start_time": "2022-11-15T14:43:54.920Z"
   },
   {
    "duration": 20,
    "start_time": "2022-11-15T14:44:47.712Z"
   },
   {
    "duration": 8,
    "start_time": "2022-11-15T14:45:20.347Z"
   },
   {
    "duration": 1534,
    "start_time": "2022-11-15T14:45:26.346Z"
   },
   {
    "duration": 2324,
    "start_time": "2022-11-15T14:45:51.160Z"
   },
   {
    "duration": 331,
    "start_time": "2022-11-15T14:45:56.760Z"
   },
   {
    "duration": 309945,
    "start_time": "2022-11-15T14:46:23.872Z"
   },
   {
    "duration": 728,
    "start_time": "2022-11-15T14:51:41.485Z"
   },
   {
    "duration": 2,
    "start_time": "2022-11-15T14:51:42.215Z"
   },
   {
    "duration": 15691,
    "start_time": "2022-11-15T14:51:42.219Z"
   },
   {
    "duration": 9787,
    "start_time": "2022-11-15T14:51:57.912Z"
   },
   {
    "duration": 4986,
    "start_time": "2022-11-15T14:52:07.704Z"
   },
   {
    "duration": 19,
    "start_time": "2022-11-15T14:52:12.692Z"
   },
   {
    "duration": 2306,
    "start_time": "2022-11-15T14:52:12.713Z"
   },
   {
    "duration": 476,
    "start_time": "2022-11-15T14:52:15.021Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.499Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.500Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.501Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.503Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.504Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.505Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.506Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.508Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-15T14:52:15.509Z"
   },
   {
    "duration": 1737,
    "start_time": "2022-11-15T14:52:59.249Z"
   },
   {
    "duration": 103,
    "start_time": "2022-11-15T14:53:12.127Z"
   },
   {
    "duration": 1000,
    "start_time": "2022-11-15T14:53:18.622Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-15T14:53:24.892Z"
   },
   {
    "duration": 838,
    "start_time": "2022-11-15T14:53:27.755Z"
   },
   {
    "duration": 2033,
    "start_time": "2022-11-15T14:53:31.251Z"
   },
   {
    "duration": 1312,
    "start_time": "2022-11-15T14:53:42.577Z"
   },
   {
    "duration": 2736,
    "start_time": "2022-11-15T14:53:46.406Z"
   },
   {
    "duration": 24,
    "start_time": "2022-11-15T14:53:58.919Z"
   },
   {
    "duration": 1413,
    "start_time": "2022-11-15T14:54:00.893Z"
   },
   {
    "duration": 2867,
    "start_time": "2022-11-15T14:54:06.140Z"
   },
   {
    "duration": 320,
    "start_time": "2022-11-15T14:54:09.077Z"
   },
   {
    "duration": 336,
    "start_time": "2022-11-15T14:54:19.746Z"
   },
   {
    "duration": 31603,
    "start_time": "2022-11-15T14:54:23.532Z"
   }
  ],
  "colab": {
   "name": "PySparkSprint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
